### Hi there üëã
(Here is my [ü™ê Blog](https://jaz201107.github.io/))



<table>
  <thead>
    <tr>
      <th>Type </th>
      <th>Paper </th>
      <th>Description </th>
      <th>Code </th>
      <th>Blog </th>
      <th>Recommend Reeading</th>
    </tr> 
  </thead>
  <tbody>
    <th colspan="6" style="text-align: center; vertical-align: middle; line-height: 50px;" > <strong>Computer Vision</strong> </th>
    <tr> 
      <td rowspan="1" style="text-align: center; vertical-align: middle;">Image Classification</td>
      <td> <a href="https://arxiv.org/abs/2010.11929" target="_blank"> An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale </td>
      <td> Vision Transformers (ViT) apply Transformer models to image recognition by processing images as patch sequences </td>
      <td> <a href="https://github.com/JAZ201107/PyTorch-DL/blob/main/ViT.ipynb" target="_blank"> ViT Jupyter Notebook </a> </td>
      <td>  </td>
      <td> ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê </td>
    </tr>
    <tr>
      <td rowspan="2" style="text-align: center; vertical-align: middle;">Image Segmentation</td>
      <td> <a href="https://arxiv.org/abs/1505.04597" target="_blank"> U-Net: Convolutional Networks for Biomedical Image Segmentation </td>
      <td> U-Net is a convolutional neural network architecture designed for biomedical image segmentation, excelling in capturing fine-grained details. </td>
      <td> <a href="https://github.com/JAZ201107/PyTorch-DL/blob/main/U_Net.ipynb" target="_blank"> U-Net Jupyter Notebook </a> </td>
      <td>  </td>
      <td> ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê </td>
      <td> <a href="https://arxiv.org/abs/1804.03999" target="_blank"> Attention U-Net: Learning Where to Look for the Pancreas </td>
      <td> Attention U-Net enhances U-Net by integrating attention mechanisms, allowing the model to focus on relevant regions for improved image segmentation.</td>
      <td> <a href="https://github.com/JAZ201107/PyTorch-DL/blob/main/U_Net.ipynb" target="_blank"> U-Net Jupyter Notebook </a> </td>
      <td>  </td>
      <td> ‚≠ê‚≠ê‚≠ê </td>
    </tr>
    <tr>
      <td>Sub Row 1-2</td>
      <td>Sub Row 1-2</td>
    </tr>
    <tr>
      <td>Row 2</td>
      <td>Regular Value</td>
      <td>Regular Value</td>
    </tr>
    <th colspan="6" style="text-align: center; vertical-align: middle; line-height: 50px;" > <strong>Natural Langue Processing</strong> </th>
    <tr> </tr>
    <th colspan="6" style="text-align: center; vertical-align: middle; line-height: 50px;" > <strong>Reinforcement Learning</strong> </th>
    <tr> </tr>
    <th colspan="6" style="text-align: center; vertical-align: middle; line-height: 50px;" > <strong>Multi Model</strong> </th>
    <tr> 
      <td> </td>
      <td> <a href="https://arxiv.org/abs/2103.00020" target="_blank"> Learning Transferable Visual Models From Natural Language Supervision </td>
      <td> 
        CLIP (<b>Contrastive</b> Language‚ÄìImage Pretraining) aligns images and text by training on diverse datasets, enabling versatile visual-text understanding 
      </td>
      <td> <a href="https://github.com/JAZ201107/PyTorch-DL/blob/main/CLIP.ipynb" target="_blank"> CLIP Jupyter Notebook </a> </td>
      <td>  </td>
      <td> ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê </td>
    </tr>
  </tbody>
</table>


